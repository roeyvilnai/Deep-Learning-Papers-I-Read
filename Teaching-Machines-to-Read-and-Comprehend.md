# Teaching Machine to Read and Comprehend

## The Goal
The authors aspired to create a system that can answer questions (query) about a given text (context) without prior world or domain-specific knowledge.  This task lacks a benchmark dataset to train on.

## Data Collection
The authors generated a large dataset that is comprised of CNN and Daily Mail articles, with summary bullets provided for each article.  The bullets are not cut-and-paste sentences from the articles, and so can be used as 'queries' for the article.  Queries are generated by replacing one entity at a time in the bullet with a placeholder.

### Entity Anonymization
To prevent the system from learning 'world-knowledge' about entities, in each article each entity is replaced by an abstract entity marker that symbolizes it (ent119, for example).  In the anonymised setup, the context document is required for answering a query, whereas in the original setup a query could theoretically be answered by someone with general knowledge of the subject matter.

## Models
The authors describe classic NLP models for handling such tasks.  I will not go into detail about them here.  More interestingly, they describe 3 neural models network models - the deep LSTM reader, the attentive reader and the impatient reader.
The neural network models try to estimate the probability of word type a from document d answering query q.  Thus, every models defines a function g(d,q) which returns a vector embedding of a document and query pair.  The probability of word a is then modelled as W(a)g(d,q), where W(a) is the row corresponding to word a in a weight matrix W.

### Deep LSTM Reader
For every context-query pair, the text is fed into an LSTM as a concatenation of context text, a special delimiter and then the query text (the order of the query and context could also be reversed).  The network then tries to predict the token that answers the query. They use a deep LSTM with skip connections from each input x(t) to every hidden layer and from every hidden layer to the output y(t).
Thus, for the Deep LSTM Reader the function g is y(|d|+|q|) - the final output of the network.

### The Attentive Reader
The Attentive Reader employs an attention mechanism, which allows it to give greater value for different parts of the context document.  This model encodes the document and query separately using bidirectional single layer LSTMs.  
The encoding u of a query q is formed by concatenating the outputs of the two LSTMs.
For the document encoding, at each timestamp the output of the LSTMs is concatenated to a single vector.  Then final encoding of the whole document r is a weighted sum of those vectors, and these weights are interpreted as the degree to which the network attends to a particular token when answering the query.
Finally, the function g for the reader is tanh(W_r*r+W_u*u). 

### The Impatient Reader
The Impatient Reader employs a different attention mechanism, that allows it to reread the document as it is reading the query.  Whenever a new query token i is read, it computes a document representation r(i).  Each r(i) is computed in part by r(i-1), and so as the query is read the reader accumulates information from the document which it will use to answer the query.  
The function g for this reader is the same at the attentive reader, only now the vector r is the vector r(i) computed at the last token of the query.

## Further Reading
Reference articles that I found interesting and worth reading:
* Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. In Proceedings of ACL, 2014.
* Ronan Collobert, JasonWeston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, November 2011.
* Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27.
* Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence. Springer, 2012.